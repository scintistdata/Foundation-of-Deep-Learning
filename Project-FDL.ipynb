{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "cell_execution_strategy": "setup",
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Model architectures and training phase\n",
        "This section deals with the models creation, the performance results and the dicussion of them.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s2QnN7tuNKAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy opencv-python matplotlib"
      ],
      "metadata": {
        "id": "MjH_OdItqXCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "id": "SHKm14iYl9o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NaRGpFrPqxd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers, models\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "zgUk7mzuszxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "zDaMhtaOsguw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "PYPiCsv3NTrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url,  cache_dir='.', untar=True)\n",
        "data_dir = pathlib.Path(data_dir)"
      ],
      "metadata": {
        "id": "U_duE__QrwV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting IMages, Labels, and class names from the directory folder"
      ],
      "metadata": {
        "id": "iqh-bPQKNZT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Define the directory path where the images are stored\n",
        "#image_dir = '/content/flower_photos/'  # Change this to the actual path\n",
        "\n",
        "# Load and preprocess the image data\n",
        "def preprocess_images(folder_path, image_size=(128,128)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = []\n",
        "\n",
        "    for class_name in os.listdir(folder_path):\n",
        "        class_folder = os.path.join(folder_path, class_name)\n",
        "        if os.path.isdir(class_folder):\n",
        "            for image_file in os.listdir(class_folder):\n",
        "                image_path = os.path.join(class_folder, image_file)\n",
        "                img = Image.open(image_path)\n",
        "                img = img.resize(image_size)\n",
        "                img_array = np.array(img)\n",
        "                images.append(img_array)\n",
        "                labels.append(class_name)\n",
        "                if class_name not in class_names:\n",
        "                    class_names.append(class_name)\n",
        "\n",
        "    # Convert the labels to numerical values using LabelEncoder\n",
        "    label_encoder = LabelEncoder()\n",
        "    Y = label_encoder.fit_transform(labels)\n",
        "    y = np.array(Y)\n",
        "\n",
        "    # Convert the list of images to a NumPy array\n",
        "    X = np.array(images)\n",
        "\n",
        "    return X, y, class_names\n",
        "\n",
        "# Load and preprocess the images and labels\n",
        "X, y, class_names = preprocess_images(data_dir)\n",
        "\n",
        "# Verify the shape of X and Y\n",
        "print(\"Shape of X:\", X.shape)  # (num_samples, 224, 242, 3) for RGB images\n",
        "print(\"Shape of Y:\", y.shape)  # (num_samples,) representing the labels as numerical values\n",
        "print(\"Class names:\", class_names)  # List of class names (folder names)\n",
        "\n",
        "# Now, you can use X and Y as input data and target data for your model.\n",
        "# The images have been resized to 224x224, and Y contains the labels represented as numerical values.\n"
      ],
      "metadata": {
        "id": "1eftsatAj0N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocesisng Data"
      ],
      "metadata": {
        "id": "mUGS6yW1C-GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=X/255        # scaling of each image pixel"
      ],
      "metadata": {
        "id": "xj4uYkmGj0Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Here We are performing Splitting Data into Train , Test and Validation. Data Augmentation"
      ],
      "metadata": {
        "id": "9TJ10aBEDc9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install keras-tuner"
      ],
      "metadata": {
        "id": "hhfwMt4j9ipR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=0)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train, random_state=0)\n",
        "\n",
        "# Create a Sequential model for data augmentation\n",
        "#data_augmentation = keras.Sequential([\n",
        "#    layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=x_train.shape[1:]),\n",
        "#    layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "#    layers.experimental.preprocessing.RandomZoom(0.2),\n",
        "#    layers.experimental.preprocessing.RandomContrast(0.2),\n",
        "#     layers.experimental.preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
        "#])\n",
        "\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=x_train.shape[1:]),\n",
        "    layers.experimental.preprocessing.RandomRotation(factor=0.1),\n",
        "    layers.experimental.preprocessing.RandomZoom(height_factor=0.1, width_factor=0.1),\n",
        "])\n",
        "\n",
        "# Create TensorFlow datasets for training, validation, and testing\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "# Add a batch dimension to the input data before applying data augmentation\n",
        "train_dataset = train_dataset.map(lambda x, y: (tf.expand_dims(x, 0), y))\n",
        "validation_dataset = validation_dataset.map(lambda x, y: (tf.expand_dims(x, 0), y))\n",
        "\n",
        "# Apply data augmentation to the training dataset\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(x_train))\n",
        "train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "train_dataset = train_dataset.batch(batch_size=16)\n",
        "\n",
        "# Apply data augmentation to the validation dataset (optional)\n",
        "#validation_dataset = validation_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "#validation_dataset = validation_dataset.batch(batch_size=16)\n",
        "\n",
        "# Now, you can use the augmented training and validation datasets in your model training\n"
      ],
      "metadata": {
        "id": "GEpRRF3lepv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display Few Augmeneted Random Images"
      ],
      "metadata": {
        "id": "5D3qZL_mDvIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "for batch in train_dataset.take(1):\n",
        "    augmented_images = batch[0]  # First element in the batch is the augmented images\n",
        "    original_labels = batch[1]   # Second element in the batch is the corresponding labels\n",
        "\n",
        "# Convert the random indices to a TensorFlow tensor\n",
        "num_images_to_visualize = 5\n",
        "random_indices = np.random.choice(len(augmented_images), num_images_to_visualize, replace=False)\n",
        "random_indices_tensor = tf.constant(random_indices)\n",
        "\n",
        "# Select the images at the random indices for visualization\n",
        "selected_images = tf.gather(augmented_images, random_indices_tensor)\n",
        "\n",
        "# Create a grid to visualize the images\n",
        "fig, axes = plt.subplots(1, num_images_to_visualize, figsize=(15, 3))\n",
        "\n",
        "# Plot the images\n",
        "for i, image in enumerate(selected_images):\n",
        "    image = tf.squeeze(image)  # Remove the batch dimension (squeeze)\n",
        "    image = (image.numpy() * 255).astype(np.uint8)  # Scale the image back to [0, 255]\n",
        "    axes[i].imshow(image)\n",
        "    axes[i].set_title(f\"Augmented Image {i+1}\")\n",
        "    axes[i].axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tpADZS1a8hjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model 1"
      ],
      "metadata": {
        "id": "8OfEycJaDuKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 5\n",
        "model01 = keras.Sequential([\n",
        "      # Include data augmentation as the first layer\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=x_train.shape[1:]),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dropout(0.5),  # Dropout layer to reduce overfitting\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model01.compile(optimizer= keras.optimizers.Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "282AFFIZ-Daw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model01.summary()"
      ],
      "metadata": {
        "id": "8fQ61GhvKT4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a generator function for training data"
      ],
      "metadata": {
        "id": "JSoYKmUMM6P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def data_generator(x_train, y_train, batch_size):\n",
        "    num_samples = len(x_train)\n",
        "    indices = np.arange(num_samples)\n",
        "\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch_indices = indices[i:i + batch_size]\n",
        "            batch_x = x_train[batch_indices]\n",
        "            batch_y = y_train[batch_indices]\n",
        "            yield batch_x, batch_y\n",
        "\n",
        "# Define batch size and create the data generator\n",
        "batch_size = 16\n",
        "train_generator = data_generator(x_train, y_train, batch_size)\n",
        "\n",
        "# Fit the model using the data generator and include validation data\n",
        "steps_per_epoch = len(x_train) // batch_size\n",
        "validation_dataset = (x_val, y_val)  # Use the validation data you already have\n",
        "\n",
        "model01.fit(train_generator, epochs=30, steps_per_epoch=steps_per_epoch, validation_data=validation_dataset)\n"
      ],
      "metadata": {
        "id": "7XvH83E0-Ds4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tf.keras.utils.plot_model(model01,show_shapes=True,show_layer_names=True)"
      ],
      "metadata": {
        "id": "k0GqLgKEf2wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot training and validation Accuracy AND Loss over epochs"
      ],
      "metadata": {
        "id": "C5WM9yFxNFB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "history = model01.history.history\n",
        "\n",
        "# Plot training and validation loss over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "n8isi9uzqWr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assuming you have already trained the model and have x_test and y_test ready\n",
        "\n",
        "# Evaluate the model on the test dataset"
      ],
      "metadata": {
        "id": "3AgvyGUTEYHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_loss, test_accuracy = model01.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "9mSBnZQ9-Duh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make Prediction on train data for Target"
      ],
      "metadata": {
        "id": "06H4MWTEEcap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "num_images_to_predict = 5\n",
        "random_indices = np.random.choice(len(x_test), num_images_to_predict, replace=False)\n",
        "selected_images = x_test[random_indices]\n",
        "\n",
        "# Make predictions on the selected images\n",
        "predictions = model01.predict(selected_images)\n",
        "\n",
        "# Convert predictions to class labels (if you have a classification problem)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Display the predictions\n",
        "for i in range(num_images_to_predict):\n",
        "    print(f\"Image {i+1}:\")\n",
        "    print(f\"True Label: {y_test[random_indices[i]]}\")\n",
        "    print(f\"Predicted Class: {predicted_classes[i]}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "X1Bzt0wh-Dwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing some randomly chosen indices from the test datasett\n",
        "\n"
      ],
      "metadata": {
        "id": "VGIZ2e-zE5ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_images_to_predict = 5\n",
        "random_indices = np.random.choice(len(x_test), num_images_to_predict, replace=False)\n",
        "selected_images = x_test[random_indices]\n",
        "true_labels = y_test[random_indices]\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Create a mapping for class labels (if applicable)\n",
        "\n",
        "# Display the images with true and predicted labels\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(num_images_to_predict):\n",
        "    plt.subplot(1, num_images_to_predict, i + 1)\n",
        "    plt.imshow(selected_images[i])\n",
        "    plt.title(f\"True: {class_names[true_labels[i]]}\\nPredicted: {class_names[predicted_classes[i]]}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8_XB9iZV-Dy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make predictions on the test dataset using the trained Keras model"
      ],
      "metadata": {
        "id": "G5gccJWMFyJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make predictions on the test dataset using the trained Keras model\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming you have already trained the model and have x_test, y_test, and predicted_classes ready\n",
        "\n",
        "predictions = model1.predict(x_test)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print the classification report\n",
        " # Replace with your class names\n",
        "print(classification_report(y_test, predicted_classes, target_names=class_names))\n"
      ],
      "metadata": {
        "id": "AyT4mMX1dZSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EqVLNc5Uj0b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model2 with regularization, Batch normalization and double Dense layer"
      ],
      "metadata": {
        "id": "jXaGPbTzErsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import regularizers\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "num_classes = 5\n",
        "model2 = keras.Sequential([\n",
        "    data_augmentation,  # Include data augmentation as the first layer\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=x_train.shape[1:], padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "\n",
        "#    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "#    layers.BatchNormalization(),\n",
        "#    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model2.compile(optimizer= keras.optimizers.Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "6RIhS4quEtCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a new possibilitty UP||"
      ],
      "metadata": {
        "id": "DGkXmVvEqVDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import regularizers\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "num_classes = 5\n",
        "model02 = keras.Sequential([\n",
        "    data_augmentation,  # Include data augmentation as the first layer\n",
        "    layers.Conv2D(64, (3,3), activation='relu', input_shape=x_train.shape[1:], padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "\n",
        "#    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "#    layers.BatchNormalization(),\n",
        "#    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model02.compile(optimizer= keras.optimizers.Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "1GuSaIQSjjba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2bhnN5UadxNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "hDNJrVrNEtPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJ60xVxXHm9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(x_train, y_train, batch_size):\n",
        "    num_samples = len(x_train)\n",
        "    indices = np.arange(num_samples)\n",
        "\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch_indices = indices[i:i + batch_size]\n",
        "            batch_x = x_train[batch_indices]\n",
        "            batch_y = y_train[batch_indices]\n",
        "            yield batch_x, batch_y\n",
        "\n",
        "# Define batch size and create the data generator\n",
        "batch_size = 16\n",
        "train_generator = data_generator(x_train, y_train, batch_size)\n",
        "\n",
        "# Fit the model using the data generator and include validation data\n",
        "steps_per_epoch = len(x_train) // batch_size\n",
        "validation_dataset = (x_val, y_val)  # Use the validation data you already have\n",
        "\n",
        "model2.fit(train_generator, epochs=30, steps_per_epoch=steps_per_epoch, validation_data=validation_dataset)\n"
      ],
      "metadata": {
        "id": "lYe6Q-bQEtR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "TNxeROTKqacb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tf.keras.utils.plot_model(model2,show_shapes=True,show_layer_names=True)\n"
      ],
      "metadata": {
        "id": "nTHgPz1Vi39t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # Plot training and validation Accuracy AND Loss over epochs"
      ],
      "metadata": {
        "id": "dMpVOcizG7WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "history = model2.history.history\n",
        "\n",
        "# Plot training and validation loss over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CMaKOk6LdoSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vNEglMxXmc1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WVSP0Ux_mfrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the training history from the model\n",
        "history = model02.history.history\n",
        "\n",
        "# Plot training and validation loss over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3eVsD-9yEtUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the plot of the accuracy, you can see that the model could probably be trained a little more as the trend for accuracy on both datasets is still rising for the last few epochs. You can also see that the model has not yet over-learned the training dataset, showing comparable skill on both datasets."
      ],
      "metadata": {
        "id": "vzLk0GKAG5Aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate the model on the test dataset"
      ],
      "metadata": {
        "id": "_unnPNHtHaO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_loss, test_accuracy = model2.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "rZ_OV3t6HHJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make Prediction on train data for Target"
      ],
      "metadata": {
        "id": "q7717FLuHkIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "num_images_to_predict = 5\n",
        "random_indices = np.random.choice(len(x_test), num_images_to_predict, replace=False)\n",
        "selected_images = x_test[random_indices]\n",
        "\n",
        "# Make predictions on the selected images\n",
        "predictions = model2.predict(selected_images)\n",
        "\n",
        "# Convert predictions to class labels (if you have a classification problem)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Display the predictions\n",
        "for i in range(num_images_to_predict):\n",
        "    print(f\"Image {i+1}:\")\n",
        "    print(f\"True Label: {y_test[random_indices[i]]}\")\n",
        "    print(f\"Predicted Class: {predicted_classes[i]}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "YRgrU4ShHHMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize some randomly chosen indices from the test dataset"
      ],
      "metadata": {
        "id": "xQ1MHvXFINCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "num_images_to_predict = 5\n",
        "random_indices = np.random.choice(len(x_test), num_images_to_predict, replace=False)\n",
        "selected_images = x_test[random_indices]\n",
        "true_labels = y_test[random_indices]\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "# Create a mapping for class labels (if applicable)\n",
        "\n",
        "# Display the images with true and predicted labels\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(num_images_to_predict):\n",
        "    plt.subplot(1, num_images_to_predict, i + 1)\n",
        "    plt.imshow(selected_images[i])\n",
        "    plt.title(f\"True: {class_names[true_labels[i]]}\\nPredicted: {class_names[predicted_classes[i]]}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uc2irf3UHHPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make predictions on the test dataset using the trained Keras model"
      ],
      "metadata": {
        "id": "Oco1XYNxIegi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "predictions = model2.predict(x_test)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print the classification report\n",
        " # Replace with your class names\n",
        "print(classification_report(y_test, predicted_classes, target_names=class_names))"
      ],
      "metadata": {
        "id": "s1ArNA7mHHSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model3**"
      ],
      "metadata": {
        "id": "vH2MC2inIjKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 5\n",
        "model003 = keras.Sequential([\n",
        "    data_augmentation,\n",
        "    layers.Conv2D(64, 7, padding=\"same\"),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "\n",
        "    layers.Conv2D(64, 3, padding=\"same\"),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "\n",
        "    # Note the explicit import of Conv2D from keras.layers\n",
        "    keras.layers.Conv2D(64, 3, padding=\"same\"),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "\n",
        "    layers.MaxPooling2D(3, strides=1, padding=\"same\"),\n",
        "    layers.Conv2D(64, 3, padding=\"same\"),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.Conv2D(64, 3, padding=\"same\"),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "\n",
        "    layers.Conv2D(64, 3, activation=\"relu\"),\n",
        "    layers.GlobalMaxPooling2D(),\n",
        "    layers.Dense(256, activation=\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "\n",
        "    layers.Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model003.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "ynSUp0luEtZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTing Other possibilities"
      ],
      "metadata": {
        "id": "ip15uAK4qMqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import regularizers\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "num_classes = 5\n",
        "model3 = keras.Sequential([\n",
        "    data_augmentation,\n",
        "    layers.Conv2D(64,  (7,7), padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D(3, strides=1, padding=\"same\"),\n",
        "\n",
        "    layers.Conv2D(64,  (3,3), padding=\"same\",kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.Conv2D(128,  (3,3), padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.MaxPooling2D(3, strides=1, padding=\"same\"),\n",
        "\n",
        "    layers.Conv2D(128, (3,3), padding=\"same\",kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.Conv2D(128,  (3,3), padding=\"same\",kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation(\"relu\"),\n",
        "    layers.Conv2D(512, (3,3), activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.GlobalMaxPooling2D(),\n",
        "\n",
        "#    layers.Conv2D(512, 3, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "#    layers.BatchNormalization(),\n",
        "#    layers.Activation(\"relu\"),\n",
        "#    layers.GlobalMaxPooling2D(),\n",
        "\n",
        "    # ADD FLATTEN LAYER\n",
        "    layers.Flatten(),\n",
        "    # ADD DROPOUT LAYER\n",
        "    layers.Dropout(.5),\n",
        "\n",
        "    layers.Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001))])\n",
        "\n",
        "\n",
        "\n",
        "layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
        "\n",
        "\n",
        "model3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hAw97rZgqL10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.summary()"
      ],
      "metadata": {
        "id": "4_GlrWXWTY38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a generator function for training data\n",
        "def data_generator(x_train, y_train, batch_size):\n",
        "    num_samples = len(x_train)\n",
        "    indices = np.arange(num_samples)\n",
        "\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch_indices = indices[i:i + batch_size]\n",
        "            batch_x = x_train[batch_indices]\n",
        "            batch_y = y_train[batch_indices]\n",
        "            yield batch_x, batch_y\n",
        "\n",
        "# Define batch size and create the data generator\n",
        "batch_size = 16\n",
        "train_generator = data_generator(x_train, y_train, batch_size)\n",
        "\n",
        "# Fit the model using the data generator and include validation data\n",
        "steps_per_epoch = len(x_train) // batch_size\n",
        "validation_dataset = (x_val, y_val)  # Use the validation data you already have\n",
        "\n",
        "model003.fit(train_generator, epochs=30, steps_per_epoch=steps_per_epoch, validation_data=validation_dataset)\n"
      ],
      "metadata": {
        "id": "H8bbPe8pEtW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot training and validation Accuracy AND Loss over epochs"
      ],
      "metadata": {
        "id": "vPsUOb2qJAPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the training history from the model\n",
        "history = model003.history.history\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Tvhod72fEtcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate the model on the test dataset"
      ],
      "metadata": {
        "id": "X048iTSqJO-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_loss, test_accuracy = model3.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "jIFYqh6OJTSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make Prediction on train data for Target"
      ],
      "metadata": {
        "id": "2bUNJ7inJndC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get some randomly chosen indices from the test dataset\n",
        "num_images_to_predict = 5\n",
        "random_indices = np.random.choice(len(x_test), num_images_to_predict, replace=False)\n",
        "selected_images = x_test[random_indices]\n",
        "\n",
        "# Make predictions on the selected images\n",
        "predictions = model3.predict(selected_images)\n",
        "\n",
        "# Convert predictions to class labels (if you have a classification problem)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Display the predictions\n",
        "for i in range(num_images_to_predict):\n",
        "    print(f\"Image {i+1}:\")\n",
        "    print(f\"True Label: {y_test[random_indices[i]]}\")\n",
        "    print(f\"Predicted Class: {predicted_classes[i]}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "N9C9ShGAJkQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make predictions on the test dataset using the trained Keras model"
      ],
      "metadata": {
        "id": "9_NO0C8vJyh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming you have already trained the model and have x_test, y_test, and predicted_classes ready\n",
        "\n",
        "predictions = model3.predict(x_test)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print the classification report\n",
        " # Replace with your class names\n",
        "print(classification_report(y_test, predicted_classes, target_names=class_names))"
      ],
      "metadata": {
        "id": "B6-fTGp2JkTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model4"
      ],
      "metadata": {
        "id": "_1ZDlnbcKNGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "num_classes = 5\n",
        "model4 = keras.Sequential([\n",
        "data_augmentation,\n",
        "\n",
        "layers.Conv2D(32, 3, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "layers.BatchNormalization(),\n",
        "layers.Activation(\"relu\"),\n",
        "layers.MaxPooling2D(3, strides=3, padding=\"same\"),\n",
        "\n",
        "layers.Conv2D(64, 3, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "layers.BatchNormalization(),\n",
        "layers.Activation(\"relu\"),\n",
        "layers.MaxPooling2D(3, strides=3, padding=\"same\"),\n",
        "\n",
        "layers.Conv2D(128, 3, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "layers.BatchNormalization(),\n",
        "layers.Activation(\"relu\"),\n",
        "layers.MaxPooling2D(3, strides=3, padding=\"same\"),\n",
        "\n",
        "layers.Conv2D(128, 3, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "layers.BatchNormalization(),\n",
        "layers.Activation(\"relu\"),\n",
        "layers.MaxPooling2D(3, strides=3, padding=\"same\"),\n",
        "\n",
        "layers.Conv2D(512, 3, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "\n",
        "layers.BatchNormalization(),\n",
        "layers.Activation(\"relu\"),\n",
        "layers.GlobalMaxPooling2D(),\n",
        "\n",
        "# ADD FLATTEN LAYER\n",
        "layers.Flatten(),\n",
        "# ADD DROPOUT LAYER\n",
        "layers.Dropout(.15),\n",
        "\n",
        "layers.Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "layers.Dropout(.15),\n",
        "\n",
        "\n",
        "layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=tf.keras.regularizers.l2(0.001))])\n",
        "\n",
        "\n",
        "# PRINT THE SUMMARY\n",
        "model4.summary()\n",
        "\n",
        "\n",
        "model4.compile(optimizer= keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\"\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M1LoWyL9EtfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import regularizers\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "num_classes = 5\n",
        "model_large_cnn = keras.Sequential([\n",
        "    data_augmentation,  # Include data augmentation as the first layer\n",
        "\n",
        "    # First convolutional block\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=x_train.shape[1:], padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(3, strides=3, padding=\"same\"),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Second convolutional block\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(3, strides=3, padding=\"same\"),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Third convolutional block\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(3, strides=3, padding=\"same\"),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Fourth convolutional block\n",
        "    layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    #layers.MaxPooling2D(3, strides=3, padding=\"same\"),\n",
        "    layers.GlobalMaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "\n",
        "    # Flatten the output and feed into dense layers\n",
        "    layers.Flatten(),\n",
        "    # First fully connected block\n",
        "    layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Second fully connected block\n",
        "    layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Output layer\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model_large_cnn.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Cy30ZSignQVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_large_cnn.summary()"
      ],
      "metadata": {
        "id": "nRmxAXSCrHdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a generator function for training data\n",
        "def data_generator(x_train, y_train, batch_size):\n",
        "    num_samples = len(x_train)\n",
        "    indices = np.arange(num_samples)\n",
        "\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch_indices = indices[i:i + batch_size]\n",
        "            batch_x = x_train[batch_indices]\n",
        "            batch_y = y_train[batch_indices]\n",
        "            yield batch_x, batch_y\n",
        "\n",
        "# Define batch size and create the data generator\n",
        "batch_size = 16\n",
        "train_generator = data_generator(x_train, y_train, batch_size)\n",
        "\n",
        "# Fit the model using the data generator and include validation data\n",
        "steps_per_epoch = len(x_train) // batch_size\n",
        "validation_dataset = (x_val, y_val)\n",
        "\n",
        "model_large_cnn.fit(train_generator, epochs=30, steps_per_epoch=steps_per_epoch, validation_data=validation_dataset)\n"
      ],
      "metadata": {
        "id": "XVeMQdR9EthQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot training and validation Accuracy AND Loss over epochs"
      ],
      "metadata": {
        "id": "_paGATyuKmJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "history = model_large_cnn.history.history\n",
        "\n",
        "# Plot training and validation loss over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0o0TMYEQEtkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate the model on the test dataset"
      ],
      "metadata": {
        "id": "CmBVGDO5K6rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_loss, test_accuracy = model_large_cnn.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "LOucP4R7Etnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make Prediction on train data for Target\n"
      ],
      "metadata": {
        "id": "Tv2y-6YjLEAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "num_images_to_predict = 5\n",
        "random_indices = np.random.choice(len(x_test), num_images_to_predict, replace=False)\n",
        "selected_images = x_test[random_indices]\n",
        "\n",
        "# Make predictions on the selected images\n",
        "predictions = model_large_cnn.predict(selected_images)\n",
        "\n",
        "# Convert predictions to class labels (if you have a classification problem)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Display the predictions\n",
        "for i in range(num_images_to_predict):\n",
        "    print(f\"Image {i+1}:\")\n",
        "    print(f\"True Label: {y_test[random_indices[i]]}\")\n",
        "    print(f\"Predicted Class: {predicted_classes[i]}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "O03fuFx3EtqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make predictions on the test dataset using the trained Keras model\n"
      ],
      "metadata": {
        "id": "tXE-w4ZZLLnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "predictions = model_large_cnn.predict(x_test)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print the classification report\n",
        " # Replace with your class names\n",
        "print(classification_report(y_test, predicted_classes, target_names=class_names))\n"
      ],
      "metadata": {
        "id": "THU5yTpbEttu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "onzTttyTzbBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transfer Learning VGG16 PRE-TRAINED MODEL**"
      ],
      "metadata": {
        "id": "aA4Z6FHoPUTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### TRANSFORM LEARNING FROM VGG16 PRE-TRAINED MODEL"
      ],
      "metadata": {
        "id": "6VKLR_QCzbEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=x_train.shape[1:])\n"
      ],
      "metadata": {
        "id": "AyQ3UTL_zbHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n"
      ],
      "metadata": {
        "id": "8yA6HBuLzbJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "\n",
        "\n",
        "custom_head = model_large_cnn.layers[-7:]\n",
        "\n",
        "# Extract the output tensor from the base model\n",
        "base_model_output = model_large_cnn.layers[-8].output  # Assuming the last layer before dropout is the actual output\n",
        "\n",
        "# Apply the custom head to the base model output\n",
        "transfer_model = base_model_output\n",
        "for layer in custom_head:\n",
        "    transfer_model = layer(transfer_model)\n",
        "\n",
        "# Create the transfer learning model\n",
        "transfer_model = Model(inputs=model_large_cnn.input, outputs=transfer_model)\n"
      ],
      "metadata": {
        "id": "r9NgdLdrzbMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transfer_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "0kV2xbGEzbPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure to have x_train, y_train, x_val, y_val prepared accordingly\n",
        "\n",
        "# Fit the transfer learning model with your data\n",
        "transfer_model.fit(train_generator,\n",
        "                   steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                   epochs=30,\n",
        "                   validation_data=(x_val, y_val))\n"
      ],
      "metadata": {
        "id": "h4x8hSUWj0fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = transfer_model.history.history\n",
        "\n",
        "# Plot training and validation loss over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.ylim(0, max(history['loss'] + history['val_loss']))  # Set y-axis limits to start from 0\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylim(0, 1)  # Set y-axis limits for accuracy between 0 and 1\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xBJ-qmvKj0ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transfer_model.summary())"
      ],
      "metadata": {
        "id": "5B9Oox_7j0ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `***RESTNET Transfer Learning***`"
      ],
      "metadata": {
        "id": "M0W5WhK4GlcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import ResNet50\n",
        "\n",
        "# Load the pre-trained ResNet50 model without the top (fully connected) layers\n",
        "ResNet50 = ResNet50(weights='imagenet', include_top=False, input_shape=x_train.shape[1:])\n"
      ],
      "metadata": {
        "id": "sLKs32tBj0nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in ResNet50.layers:\n",
        "    layer.trainable = False\n"
      ],
      "metadata": {
        "id": "fsiPOlkgDo2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "\n",
        "custom_head = model_large_cnn.layers[-7:]\n",
        "\n",
        "# Extract the output tensor from the base model\n",
        "base_model_output = model_large_cnn.layers[-8].output\n",
        "\n",
        "# Apply the custom head to the base model output\n",
        "ResNet50_Model = base_model_output\n",
        "for layer in custom_head:\n",
        "    ResNet50_Model = layer(ResNet50_Model)\n",
        "\n",
        "# Create the transfer learning model\n",
        "ResNet50_Model = Model(inputs=model_large_cnn.input, outputs=ResNet50_Model)\n",
        "\n"
      ],
      "metadata": {
        "id": "EYtpdCHBDo41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ResNet50_Model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "HanD1_sSDo7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2hGxwuZZIoK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fit the transfer learning model with your data\n",
        "ResNet50_Model.fit(train_generator,\n",
        "                   steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                   epochs=30,\n",
        "                   validation_data=(x_val, y_val))\n"
      ],
      "metadata": {
        "id": "M3BFFKChDo-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ResNet50_Model.summary())"
      ],
      "metadata": {
        "id": "JXsbazC5DpBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = ResNet50_Model.history.history\n",
        "\n",
        "# Plot training and validation loss over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.ylim(0, max(history['loss'] + history['val_loss']))  # Set y-axis limits to start from 0\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylim(0, 1)  # Set y-axis limits for accuracy between 0 and 1\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fzRyq_XHDpDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DenseNet Pre-trained Model**"
      ],
      "metadata": {
        "id": "9woZp4e9Rz7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import DenseNet121\n",
        "\n",
        "densenet_model = DenseNet121(weights='imagenet', include_top=False, input_shape=x_train.shape[1:])\n"
      ],
      "metadata": {
        "id": "VJaYuoNrDpGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kKVW3yBuTCAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "\n",
        "custom_head = model_large_cnn.layers[-7:]\n",
        "\n",
        "# Extract the output tensor from the base model\n",
        "base_model_output = model_large_cnn.layers[-8].output\n",
        "\n",
        "# Applying the custom head to the base model output\n",
        "densenet_model = base_model_output\n",
        "for layer in custom_head:\n",
        "    densenet_model = layer(densenet_model)\n",
        "\n",
        "# Create the transfer learning model\n",
        "densenet_model = Model(inputs=model_large_cnn.input, outputs=densenet_model)\n",
        "\n",
        "densenet_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "1rx3okE3DpL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fit the transfer learning model with your data\n",
        "densenet_model.fit(train_generator,\n",
        "                   steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                   epochs=30,\n",
        "                   validation_data=(x_val, y_val))\n"
      ],
      "metadata": {
        "id": "6E5RKJmPDpOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "history = densenet_model.history.history\n",
        "\n",
        "# Plot training and validation loss over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.ylim(0, max(history['loss'] + history['val_loss']))  # Set y-axis limits to start from 0\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylim(0, 1)  # Set y-axis limits for accuracy between 0 and 1\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cDSXDLrZDpRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "y_pred = densenet_model.predict(x_val)\n",
        "\n",
        "# Convert the predicted probabilities to class labels\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_val, y_pred_labels)\n",
        "\n",
        "classes = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot(cmap='viridis', values_format='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iLpcVcsVDpUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PasfdmNVDpWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p6mb81o-DpZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5xcOuAeDpcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "FvbByUDhj0o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "JpSNZi_IinIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "id": "IDLwME7DaqHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tf.config.experimental.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "id": "ea5V_k5HaqKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "  tf.config.experimental.set_memory_growth(gpu,True)"
      ],
      "metadata": {
        "id": "caHStEQraqNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "_XmmYi_XaqQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nDh10U6EaqTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JMvYeNYLaqV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SHfd5fAeaqZX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}